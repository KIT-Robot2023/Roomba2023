{"cells":[{"cell_type":"markdown","metadata":{},"source":["## LINE通知Bot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#LINE BOT  ON\n","\n","import requests\n","\n","class LINENotifyBot(object):\n","    API_URL = 'https://notify-api.line.me/api/notify'\n","    def __init__(self, access_token):\n","        self.__headers = {'Authorization': 'Bearer ' + access_token}\n","\n","    def send(\n","        self,\n","        message,\n","        image=None,\n","        sticker_package_id=None,\n","        sticker_id=None,\n","    ):\n","        payload = {\n","            'message': message,\n","            'stickerPackageId': sticker_package_id,\n","            'stickerId': sticker_id,\n","        }\n","        files = {}\n","        if image != None:\n","            files = {'imageFile': open(image, 'rb')}\n","        r = requests.post(\n","            LINENotifyBot.API_URL,\n","            headers=self.__headers,\n","            data=payload,\n","            files=files,\n","        )\n","print(\"LINE BOTを起動します.\")"]},{"cell_type":"markdown","metadata":{},"source":["# DQN\n","参考：https://book.mynavi.jp/manatee/detail/id=89831"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorboard 設定\n","ex. tensorboard --logdir [logs へのパス]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# log_dirでlogのディレクトリを指定\n","writer = SummaryWriter(log_dir=\"./logs\")"]},{"cell_type":"markdown","metadata":{},"source":["## 準備"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import shutil\n","\n","target_dir = 'img'\n","\n","shutil.rmtree(target_dir)\n","os.mkdir(target_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# namedtupleを生成\n","from collections import namedtuple\n"," \n","Transition = namedtuple(\n","    'Transition', ('state', 'action', 'next_state', 'reward'))"]},{"cell_type":"markdown","metadata":{},"source":["## Roomba simulator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import cv2\n","\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","\n","class RoombaSimulator:\n","    def __init__(self):\n","        self.world_pos_position = [0, 0]  # 初期位置 [x, y]\n","        self.world_pos_position_pre = [0, 0]\n","        self.ball_pos_position = [0, 0]  # ボールの初期位置 [x, y]\n","        self.odometry_position = [0, 0]  # オドメトリ\n","        self.odometry_position_pre = [0, 0]\n","\n","        self.world_orientation = 0   # 初期姿勢 (rad)\n","        self.orientation = 0\n","\n","        self.left_wheel_rotation = 0  # 左車輪の回転角度 (rad)\n","        self.right_wheel_rotation = 0  # 右車輪の回転角度 (rad)\n","\n","        self.wheel_radius = 0.072  # 車輪の半径 (50mmをmに変換)\n","        self.tread = 0.235  # トレッド\n","        self.size = 0.165 # Roombaの半径\n","\n","        self.one_side_of_area = 10 #行動環境の一辺の長さ\n","\n","        self.delta_x = 0\n","        self.delta_y = 0\n","        self.delta_theta = 0\n","\n","        self.vertex1 = (0,0)\n","        self.vertex2 = (0,0)\n","        self.vertex3 = (0,0)\n","        self.vertex4 = (0,0)\n","\n","        self.animation = []\n","        self.out_img = []\n","\n","        self.count = 0\n","\n","        self.fig = plt.figure(figsize=(10,10))\n","\n","    def random_pos(self):\n","        self.ball_pos_position = [0, 0]\n","        self.world_pos_position = [0, 0]\n","\n","        while self.world_pos_position[0]-self.ball_pos_position[0]  <= 0.1 and self.world_pos_position[1]-self.ball_pos_position[1] <= 0.1:\n","\n","            self.world_pos_position = [\n","                random.uniform(-self.one_side_of_area/2, self.one_side_of_area/2),\n","                random.uniform(-self.one_side_of_area/2, self.one_side_of_area/2)]  # 初期位置 [x, y]\n","            # self.ball_pos_position = [random.uniform(-self.one_side_of_area/2, self.one_side_of_area/2), random.uniform(-self.one_side_of_area/2, self.one_side_of_area/2)]  # ボールの初期位置 [x, y]\n","            self.world_orientation = random.uniform(-3.14, 3.14)\n","\n","    def init_pos(self):\n","        self.ball_pos_position = [4, 0]\n","        self.world_pos_position = [0, 0]\n","        self.world_orientation = 0\n","\n","        options_list = [[0,0], [0,3.85], [0,-3.85]]\n","        self.world_pos_position = random.choice(options_list)\n","        self.world_pos_position = [0,0]\n","\n","\n","    def odometry_update(self, left_wheel_rotation_delta, right_wheel_rotation_delta):\n","        # 車輪の回転角度を更新\n","        self.left_wheel_rotation += left_wheel_rotation_delta\n","        self.right_wheel_rotation += right_wheel_rotation_delta\n","\n","        # Roombaの位置と姿勢を更新\n","        left_wheel_distance = left_wheel_rotation_delta * self.wheel_radius\n","        right_wheel_distance = right_wheel_rotation_delta * self.wheel_radius\n","\n","        delta_distance = (left_wheel_distance + right_wheel_distance) / 2\n","        self.delta_theta = (right_wheel_distance - left_wheel_distance) / self.tread\n","\n","        self.delta_x = delta_distance * np.cos(self.orientation)\n","        self.delta_y = delta_distance * np.sin(self.orientation)\n","\n","        self.odometry_position[0] += self.delta_x\n","        self.odometry_position[1] += self.delta_y\n","        self.orientation += self.delta_theta\n","\n","\n","    def world_pos_update(self):\n","        # Roombaのワールド座標系での移動を計算\n","        world_delta_x = np.cos(self.world_orientation) * self.delta_x - np.sin(self.world_orientation) * self.delta_y\n","        world_delta_y = np.sin(self.world_orientation) * self.delta_x + np.cos(self.world_orientation) * self.delta_y\n","\n","        # Roombaのワールド座標系での位置を更新\n","        self.world_pos_position[0] += world_delta_x\n","        self.world_pos_position[1] += world_delta_y\n","\n","        #壁に突っ込んだら，座標を更新しない．\n","        if abs(self.world_pos_position[0])+self.size < self.one_side_of_area/2 and\\\n","           abs(self.world_pos_position[1])+self.size < self.one_side_of_area/2:\n","\n","            self.world_pos_position_pre[0] = self.world_pos_position[0]\n","            self.world_pos_position_pre[1] = self.world_pos_position[1]\n","\n","            self.odometry_position_pre[0] = self.odometry_position[0]\n","            self.odometry_position_pre[1] = self.odometry_position[1]\n","\n","        else:\n","            self.world_pos_position[0] = self.world_pos_position_pre[0]\n","            self.world_pos_position[1] = self.world_pos_position_pre[1]\n","\n","            self.odometry_position[0] = self.odometry_position_pre[0]\n","            self.odometry_position[1] = self.odometry_position_pre[1]\n","\n","        # Roombaのワールド座標系での回転を更新\n","        self.world_orientation += self.delta_theta\n","\n","        '''ホモグラフィ変換された視野'''\n","        trapezoid_length_Top_side = 0.2  # 台形の下辺の長さ\n","        trapezoid_length_Bottom_side = 0.5  # 台形の下辺の長さ\n","        trapezoid_height = 0.5  # 台形の高さ\n","\n","        top_side_offset_Xcom = trapezoid_length_Top_side/2 * np.cos(self.world_orientation + np.pi/2)\n","        top_side_offset_Ycom = trapezoid_length_Top_side/2 * np.sin(self.world_orientation + np.pi/2)\n","        bottom_side_offset_Xcom = trapezoid_length_Bottom_side/2 * np.cos(self.world_orientation + np.pi/2)\n","        bottom_side_offset_Ycom = trapezoid_length_Bottom_side/2 * np.sin(self.world_orientation + np.pi/2)\n","\n","        height_offset_Xcom = trapezoid_height * np.cos(self.world_orientation)\n","        height_offset_Ycom = trapezoid_height * np.sin(self.world_orientation)\n","\n","        size_offset_Xcom = self.size * np.cos(self.world_orientation)\n","        size_offset_Ycom = self.size * np.sin(self.world_orientation)\n","\n","        # 台形の頂点座標\n","        # 左端\n","        self.vertex1 = (self.world_pos_position[0] -  top_side_offset_Xcom + size_offset_Xcom,\n","                   self.world_pos_position[1] -  top_side_offset_Ycom + size_offset_Ycom)\n","\n","        # 左上\n","        self.vertex4 = ((self.world_pos_position[0] -  bottom_side_offset_Xcom) + height_offset_Xcom,\n","                   (self.world_pos_position[1] -  bottom_side_offset_Ycom) + height_offset_Ycom)\n","\n","        # 右端\n","        self.vertex2 = (self.world_pos_position[0] +  top_side_offset_Xcom + size_offset_Xcom,\n","                   self.world_pos_position[1] +  top_side_offset_Ycom + size_offset_Ycom)\n","\n","        # 右上\n","        self.vertex3 = ((self.world_pos_position[0] +  bottom_side_offset_Xcom) + height_offset_Xcom,\n","                   (self.world_pos_position[1] +  bottom_side_offset_Ycom) + height_offset_Ycom)\n","\n","    def plot(self):\n","        # Roombaの位置と姿勢をMatplotlibでプロット\n","        self.count += 1\n","\n","        fig, ax = plt.subplots(figsize=(10.0, 10.0))\n","        ax.clear()\n","        plt.clf()\n","\n","        # 青い点でRoombaの中心を表現\n","        plt.plot(self.world_pos_position[0], self.world_pos_position[1], 'bo')\n","\n","        # 赤い点でballの中心を表現\n","        plt.plot(self.ball_pos_position[0], self.ball_pos_position[1], 'ro')\n","\n","        # 半径0.165の円をプロット\n","        circle = plt.Circle((self.world_pos_position[0], self.world_pos_position[1]), 0.165, color='cyan', fill=False)\n","        plt.gca().add_patch(circle)\n","\n","        # 赤い矢印で姿勢を表現\n","        line_length = 0.3\n","        line_dx = line_length * np.cos(self.world_orientation)\n","        line_dy = line_length * np.sin(self.world_orientation) #:.2f .format(self.world_pos_position[0])\n","\n","        plt.arrow(self.world_pos_position[0], self.world_pos_position[1], line_dx, line_dy, color='red', width=0.02)\n","\n","        # 視野を描画\n","        # trapezoid = plt.Polygon([self.vertex1, self.vertex2,self.vertex3,self.vertex4], fill=False, edgecolor='green')\n","        # plt.gca().add_patch(trapezoid)\n","\n","        # print('{:.3f}'.format(self.world_pos_position[0]),\n","        #       ',','{:.3f}'.format(self.world_pos_position[1]),\n","        #       ',','{:.3f}'.format(self.world_orientation),\n","        #       ',','{:.3f}'.format(self.odometry_position[0]),\n","        #       ',','{:.3f}'.format(self.odometry_position[0]),\n","        #       ',','{:.3f}'.format(self.orientation))\n","\n","        # グラフを正方形にし、目盛りを1ずつ増加\n","        plt.axis('equal')\n","        plt.xticks(np.arange(-5, 5, 0.5))\n","        plt.yticks(np.arange(-5, 5, 0.5))\n","\n","        plt.title(\"Roomba Simulator\")\n","        plt.grid(True)\n","        # plt.show()\n","        self.fig = fig\n","\n","        '''動画作成'''\n","        self.fig.canvas.draw()\n","        image_array = np.array(self.fig.canvas.renderer.buffer_rgba())\n","        im = cv2.cvtColor(image_array, cv2.COLOR_RGBA2BGR)\n","        # print(type(im))\n","        file_name = f\"img/image_{self.count + 1:03d}.png\"\n","        cv2.imwrite(file_name, im)\n","        # self.video.write(im)\n","\n","    def is_inside_trapezoid(self):\n","        def cross_product(ax, ay, bx, by, cx, cy):\n","            return (bx - ax) * (cy - ay) - (by - ay) * (cx - ax)\n","\n","        # 連続する頂点の各ペアについて外積を計算する．\n","        cross_product_ver1to2 = cross_product(self.vertex1[0], self.vertex1[1], self.vertex2[0], self.vertex2[1], self.ball_pos_position[0], self.ball_pos_position[1])\n","        cross_product_ver2to3 = cross_product(self.vertex2[0], self.vertex2[1], self.vertex3[0], self.vertex3[1], self.ball_pos_position[0], self.ball_pos_position[1])\n","        cross_product_ver3to4 = cross_product(self.vertex3[0], self.vertex3[1], self.vertex4[0], self.vertex4[1], self.ball_pos_position[0], self.ball_pos_position[1])\n","        cross_product_ver4to1 = cross_product(self.vertex4[0], self.vertex4[1], self.vertex1[0], self.vertex1[1], self.ball_pos_position[0], self.ball_pos_position[1])\n","\n","        # ballが台形内に入っているかを判定する．\n","        if (cross_product_ver1to2 >= 0 and cross_product_ver2to3 >= 0 and\n","                cross_product_ver3to4 >= 0 and cross_product_ver4to1 >= 0) or \\\n","        (cross_product_ver1to2 <= 0 and cross_product_ver2to3 <= 0 and\n","                cross_product_ver3to4 <= 0 and cross_product_ver4to1 <= 0):\n","            return True\n","        else:\n","            return False\n","\n","    def get_world_pos_ori(self):\n","        return self.world_pos_position[0],self.world_pos_position[1],self.world_orientation\n","\n","    def get_ball_pos(self):\n","        return self.ball_pos_position[0], self.ball_pos_position[1]\n","        # if self.is_inside_trapezoid():\n","        #     return self.ball_pos_position[0], self.ball_pos_position[1]\n","        # else:\n","        #     return [-1,-1]\n","\n","    def touch_ball(self):\n","\n","\n","        if np.sqrt((self.world_pos_position[0]-self.ball_pos_position[0])**2\n","                   +(self.world_pos_position[1]-self.ball_pos_position[1])**2) <= 0.165:\n","            return True\n","        else:\n","            return False"]},{"cell_type":"markdown","metadata":{},"source":["## Roomba environment\n","参考：https://developers.agirobots.com/jp/openai-gym-custom-env/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import gymnasium as gym\n","from gymnasium import spaces\n","\n","class RoombaEnv(gym.Env):\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self):\n","        super(RoombaEnv, self).__init__()\n","        self.roomba_simulator = RoombaSimulator()\n","        pos_limit_x,pos_limit_y = [self.roomba_simulator.one_side_of_area / 2] * 2\n","\n","        self.action_space = gym.spaces.Discrete(2)       # エージェントが取りうる行動空間を定義\n","        # self.observation_space = gym.spaces.Box(\n","        # low=np.array([-pos_limit_x, -pos_limit_y, np.NINF, -pos_limit_x, -pos_limit_y], dtype=np.float32),\n","        # high=np.array([pos_limit_x, pos_limit_y, np.inf, pos_limit_x, pos_limit_y], dtype=np.float32),\n","        # shape=(5,),)\n","\n","        self.pre_distance = 0\n","        self.ini_distance = 0\n","        self.pre_theta = 0\n","        self.step_count = 0\n","\n","        self.observation_space = gym.spaces.Box(\n","        low=np.array([-pos_limit_x, -np.pi], dtype=np.float32),\n","        high=np.array([pos_limit_x, np.pi], dtype=np.float32),\n","        shape=(2,),)\n","\n","        # self.reward_range = (0,1)       # 報酬の範囲[最小値と最大値]を定義\n","\n","    def reset(self):\n","        self.roomba_simulator.init_pos()\n","        roomba_x,roomba_y,roomba_ori = self.roomba_simulator.get_world_pos_ori()\n","        ball_x,ball_y = self.roomba_simulator.get_ball_pos()\n","\n","        distance,theta = self.cal_distance_and_theta(roomba_x,roomba_y,ball_x,ball_y)\n","\n","        # obs = np.array([roomba_x,roomba_y,roomba_ori,ball_x,ball_y])\n","        self.ini_distance = distance\n","        self.pre_distance = self.ini_distance\n","        self.step_count = 0\n","\n","        obs = np.array([distance,theta])\n","\n","        return obs\n","\n","    def ctrl(self,action):\n","        # if action == 0:\n","        #     left_wheel_rotation_delta = 1 * 1e-1\n","        #     right_wheel_rotation_delta = 1 * 1e-1\n","        # elif action == 1:\n","        #     left_wheel_rotation_delta = -1 * 1e-1\n","        #     right_wheel_rotation_delta = -1 * 1e-1\n","        # elif action == 2:\n","        #     left_wheel_rotation_delta = 1 * 1e-1\n","        #     right_wheel_rotation_delta = -1 * 1e-1\n","        # elif action == 3:\n","        #     left_wheel_rotation_delta = -1 * 1e-1\n","        #     right_wheel_rotation_delta = 1 * 1e-1\n","        # elif action == 4:\n","        #     left_wheel_rotation_delta = 0\n","        #     right_wheel_rotation_delta = 1 * 1e-1\n","        # elif action == 5:\n","        #     left_wheel_rotation_delta = 1 * 1e-1\n","        #     right_wheel_rotation_delta = 0\n","        # elif action == 6:\n","        #     left_wheel_rotation_delta = 0\n","        #     right_wheel_rotation_delta = -1 * 1e-1\n","        # elif action == 7:\n","        #     left_wheel_rotation_delta = -1 * 1e-1\n","        #     right_wheel_rotation_delta = 0\n","        # elif action == 8:\n","        #     left_wheel_rotation_delta = 0\n","        #     right_wheel_rotation_delta = 0\n","\n","        if action == 0:\n","            left_wheel_rotation_delta = 1 * 1e-1\n","            right_wheel_rotation_delta = 1 * 1e-1\n","        elif action == 1:\n","            left_wheel_rotation_delta = 0#0.5 * 1e-1\n","            right_wheel_rotation_delta = 0#1 * 1e-1\n","        # elif action == 2:\n","        #     left_wheel_rotation_delta = 0.5 * 1e-1\n","        #     right_wheel_rotation_delta = -1 * 1e-1\n","\n","        # left_wheel_rotation_delta = 0.1\n","        # right_wheel_rotation_delta = 0.1\n","\n","        return left_wheel_rotation_delta,right_wheel_rotation_delta\n","\n","    def cal_distance_and_theta(self,roomba_x,roomba_y,ball_x,ball_y):\n","        distance = np.sqrt((roomba_x-ball_x)**2+(roomba_y-ball_y)**2)\n","        theta = np.arctan2((roomba_y-ball_y),(roomba_x-ball_x))\n","\n","        return distance,theta\n","\n","\n","    def step(self, action):\n","        left_wheel_rotation_delta,right_wheel_rotation_delta = self.ctrl(action)\n","        self.roomba_simulator.odometry_update(left_wheel_rotation_delta, right_wheel_rotation_delta)\n","        self.roomba_simulator.world_pos_update()\n","        roomba_x,roomba_y,roomba_ori = self.roomba_simulator.get_world_pos_ori()\n","        ball_x,ball_y = self.roomba_simulator.get_ball_pos()\n","\n","        reward = torch.FloatTensor([0])\n","        reward2 = 0.0\n","        reward1 = 0.0\n","\n","        distance,theta = self.cal_distance_and_theta(roomba_x,roomba_y,ball_x,ball_y)\n","\n","        # obs = np.array([roomba_x,roomba_y,roomba_ori,ball_x,ball_y])\n","        obs = np.array([distance,theta])\n","\n","        if self.roomba_simulator.touch_ball():\n","            done = True\n","            reward1 = 50 + 5*(1000 - self.step_count)\n","        else:\n","            done = False\n","            if self.pre_distance > distance:\n","                reward1 = 100*(self.pre_distance - distance)\n","            else:\n","                reward1 = -0.05\n","\n","            # reward2 += 2* (0.79-abs(theta))\n","        # print(reward1,done)\n","\n","        info = {'distance':distance,'theta':theta}\n","\n","        reward = torch.FloatTensor([reward1+reward2])\n","\n","        self.pre_distance = distance\n","        self.pre_theta = theta\n","        self.step_count += 1\n","\n","        # print(reward,self.pre_distance,distance)\n","\n","        return obs, reward, done, info\n","\n","    def make_anime(self):\n","        self.roomba_simulator.plot()\n","\n","\n","    # def close(self):\n","\n","    # def seed(self, seed=None):\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","gym.envs.registration.register(id='RoombaEnv-v0',entry_point=RoombaEnv)"]},{"cell_type":"markdown","metadata":{},"source":["## Config"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import nn\n","# 定数の設定\n","ENV = 'RoombaEnv-v0'  # 使用する課題名\n","MAX_STEPS = 1000  # 1試行のstep数\n","NUM_EPISODES = 200  # 最大試行回数\n","\n","GAMMA = 0.99  # 時間割引率\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 32\n","CAPACITY = 100\n","MODELS =\"\"\n","CRITERION = nn.SmoothL1Loss()\n","UPDATE_FREQ = 1000 #step\n","NOTIFI = True\n","\n","memory_loss = []\n","update_count = []\n","\n","memory_step = []\n","sum_reward = []\n","memory_reward_mean = []\n","episode_count = []\n","memory_distance = []\n","memory_theta = []"]},{"cell_type":"markdown","metadata":{},"source":["## ReplayMemory class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 経験を保存するメモリクラスを定義します\n","class ReplayMemory:\n","    def __init__(self, CAPACITY):\n","        self.capacity = CAPACITY  # メモリの最大長さ\n","        self.memory = []  # 経験を保存する変数\n","        self.index = 0  # 保存するindexを示す変数\n","\n","    def push(self, state, action, state_next, reward):\n","        \"\"\"state, action, state_next, rewardをメモリに保存します\"\"\"\n","\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)  # メモリに保存されている経験が，最大まで満たされていない場合は，新たなに枠を確保する．\n","\n","        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n","        self.memory[self.index] = Transition(state, action, state_next, reward)\n","\n","        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n","\n","    def sample(self, batch_size):\n","        \"\"\"batch_size分だけ、ランダムに保存内容を取り出します\"\"\"\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"]},{"cell_type":"markdown","metadata":{},"source":["## Brain class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# エージェントが持つ脳となるクラスです、DQNを実行\n","# Q関数をディープラーニングのネットワークをクラスとして定義\n","import random\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class Brain:\n","    def __init__(self, num_states, num_actions):\n","        self.update_time = 0\n","        self.num_states = num_states\n","        self.num_actions = num_actions\n","        self.memory = ReplayMemory(CAPACITY)\n","\n","\n","        # Online Q Network\n","        self.model = nn.Sequential()\n","        self.model.add_module('fc1', nn.Linear(self.num_states, 256))\n","        self.model.add_module('relu1', nn.ReLU())\n","        self.model.add_module('fc2', nn.Linear(256, 128))\n","        self.model.add_module('relu2', nn.ReLU())\n","        self.model.add_module('fc3', nn.Linear(128, 64))\n","        self.model.add_module('relu3', nn.ReLU())\n","        self.model.add_module('fc4', nn.Linear(64, self.num_actions))\n","\n","        # Target Q Network\n","        self.target_model = nn.Sequential()\n","        self.target_model.add_module('fc1', nn.Linear(self.num_states, 256))\n","        self.target_model.add_module('relu1', nn.ReLU())\n","        self.target_model.add_module('fc2', nn.Linear(256, 128))\n","        self.target_model.add_module('relu2', nn.ReLU())\n","        self.target_model.add_module('fc3', nn.Linear(128, 64))\n","        self.target_model.add_module('relu3', nn.ReLU())\n","        self.target_model.add_module('fc4', nn.Linear(64, self.num_actions))\n","\n","        print(self.model)  # ネットワークの形を出力\n","        MODELS = self.model\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n","\n","    def replay(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        transitions = self.memory.sample(BATCH_SIZE)\n","        batch = Transition(*zip(*transitions))\n","        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,batch.next_state)))\n","\n","        state_batch = Variable(torch.cat(batch.state))\n","        action_batch = Variable(torch.cat(batch.action))\n","        reward_batch = Variable(torch.cat(batch.reward))\n","        non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]))\n","\n","        self.model.eval()\n","        state_action_values = self.model(state_batch).gather(1, action_batch)\n","\n","        # 教師データの作成\n","        next_state_values = Variable(torch.zeros(BATCH_SIZE).type(torch.FloatTensor))\n","        next_state_values[non_final_mask] = self.target_model(non_final_next_states).data.max(1)[0]\n","        expected_state_action_values = reward_batch + GAMMA * next_state_values\n","\n","        if self.update_time % UPDATE_FREQ == 0:\n","            self.target_model.load_state_dict(self.model.state_dict())\n","\n","        # パラメータの更新\n","        self.model.train()\n","        loss = CRITERION(state_action_values, expected_state_action_values)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # 記録\n","        self.update_time +=  1\n","        writer.add_scalar(\"loss vs. step\", loss.detach().numpy(),self.update_time)\n","        memory_loss.append(loss.detach().numpy())\n","        update_count.append(self.update_time)\n","\n","\n","    def decide_action(self, state, episode):\n","        epsilon = 0.5 * (1 / (episode + 1))\n","        if epsilon < 0.1:\n","            epsilon = 0.1\n","\n","        if epsilon <= np.random.uniform(0, 1):\n","            self.model.eval()  # ネットワークを推論モードに切り替える\n","            action = self.model(Variable(state)).data.max(1)[1].view(1, 1)\n","        else:\n","            action = torch.LongTensor(\n","                [[random.randrange(self.num_actions)]])\n","\n","        return action\n","\n","    def save_model(self):\n","        # モデル全体の保存\n","        torch.save(self.model, 'models/model.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Agent class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Agent:\n","    def __init__(self, num_states, num_actions):\n","        \"\"\"課題の状態と行動の数を設定します\"\"\"\n","        self.num_states = num_states  # CartPoleは状態数4を取得\n","        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n","        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n","\n","    def update_q_function(self):\n","        \"\"\"Q関数を更新します\"\"\"\n","        self.brain.replay()\n","\n","    def get_action(self, state, step):\n","        \"\"\"行動の決定します\"\"\"\n","        action = self.brain.decide_action(state, step)\n","        return action\n","\n","    def memorize(self, state, action, state_next, reward):\n","        \"\"\"memoryオブジェクトに、state, action, state_next, rewardの内容を保存します\"\"\"\n","        self.brain.memory.push(state, action, state_next, reward)\n","\n","    def record_dqn_model(self):\n","        self.brain.save_model()"]},{"cell_type":"markdown","metadata":{},"source":["## Environment class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","\n","class Environment:\n","    def __init__(self):\n","        self.env = gym.make(ENV)  # 実行する課題を設定, render_mode=\"human\"\n","        self.num_states = self.env.observation_space.shape[0]  # 課題の状態と行動の数を設定\n","        self.num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n","        # 環境内で行動するAgentを生成\n","        self.agent = Agent(self.num_states, self.num_actions)\n","        self.reward_mean = np.zeros(10)  # 100試行分の報酬を格納し、平均報酬を求める．\n","\n","        # 動画の記録\n","        self.fig = plt.figure()\n","        self.movie = []\n","\n","        self.rewards = 0\n","        self.reward_mean_pre = 0\n","        self.episode_final = 0\n","        self.num_dizitized = 10  # 状態の分割数\n","\n","    def list_2_Tensor(self, observation):\n","        state = torch.from_numpy(observation).type(torch.FloatTensor)\n","        state = torch.unsqueeze(state, 0)\n","        #ex. [0.1, 0.2, 0.3, 0.4] → [[0.1, 0.2, 0.3, 0.4]]．\n","        #二次元のテンソルに変更し，バッチ処理できるようにする． ex.[[0.1, 0.2, 0.3, 0.4],[...]]\n","        return state\n","\n","    # 各値を離散値に変換\n","    def bins(self, clip_min, clip_max, num):\n","        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n","\n","    def digitize_state(self, observation):\n","        distance,theta = observation\n","        digitized = [\n","            np.digitize(distance, bins=self.bins(0, 5.6, self.num_dizitized)),\n","            np.digitize(theta, bins=self.bins(-0.79, 0.79, self.num_dizitized))\n","        ]\n","        # return sum([x * (self.num_dizitized**i) for i, x in enumerate(digitized)])\n","        return np.array(digitized)\n","\n","\n","    def run(self):\n","        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n","            observation = self.env.reset()  # 環境の初期化\n","            state = self.list_2_Tensor(observation)\n","\n","            for step in range(MAX_STEPS):\n","                action = self.agent.get_action(state, episode)  # 行動を求める\n","                observation_next, reward, done, info = self.env.step(action[0, 0].item())\n","                # observation_next = self.digitize_state(observation_next)\n","                state_next = self.list_2_Tensor(observation_next)\n","                self.agent.memorize(state, action, state_next, reward)\n","                self.agent.update_q_function()\n","\n","                state = state_next\n","                int_reward = int(reward.item())\n","                self.rewards += int_reward\n","\n","                if done or step == MAX_STEPS -1:\n","\n","                    self.reward_mean = np.hstack((self.reward_mean[1:], self.rewards))\n","                    print('%d Episode: Finished after %d  :10Average = %.3lf' % (episode, step + 1, self.reward_mean.mean()))\n","                    # writer.add_scalar(\"10_mean_step vs. Episode\", self.total_step.mean(),episode)\n","\n","                    episode_count.append(episode + 1)\n","                    memory_reward_mean.append(self.reward_mean.mean())\n","                    sum_reward.append(self.rewards)\n","                    memory_step.append(step+1)\n","                    memory_distance.append(info['distance'])\n","                    memory_theta.append(info['theta'])\n","\n","                    if episode == NUM_EPISODES - 1:\n","                        self.agent.record_dqn_model()\n","\n","                    break\n","\n","\n","    def inference(self):\n","        print(\"**** Inference Test ****\")\n","        observation = self.env.reset()  # 環境の初期化\n","        state = self.list_2_Tensor(observation)\n","\n","        for step in range(MAX_STEPS):\n","            action = self.agent.get_action(state, 0)  # 行動を求める\n","            observation_next, reward, done, _ = self.env.step(action[0, 0].item())\n","\n","            state_next = self.list_2_Tensor(observation_next)\n","            state = state_next\n","            self.env.make_anime()\n","\n","            print('now %d step : get reward = %d' % (step + 1, reward))\n","\n","            if done:\n","                break"]},{"cell_type":"markdown","metadata":{},"source":["# main"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# main\n","roomba_env = Environment()\n","roomba_env.run()\n","roomba_env.inference()"]},{"cell_type":"markdown","metadata":{},"source":["# プロット"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# env.close()\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.figure(tight_layout=True)\n","plt.subplot(2, 2, 1)\n","plt.plot(episode_count,memory_reward_mean, color=\"red\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"reward_mean\")\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(episode_count, memory_step, color=\"blue\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"step\")\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(episode_count,memory_distance, color=\"green\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"final_distance\")\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(episode_count, memory_theta, color=\"yellow\")\n","plt.grid()\n","plt.xlabel(\"episode\")\n","plt.ylabel(\"theta\")\n","\n","plt.savefig(\"img/test.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","import datetime\n","\n","dt_now = datetime.datetime.now()\n","\n","with open(\"./settings.json\", \"r\", encoding=\"utf-8\") as f:\n","        j = json.load(f)\n","\n","token = j[\"LINE_token\"][\"my_token\"]\n","\n","if NOTIFI:\n","        bot = LINENotifyBot(access_token = token)\n","        bot.send(\n","        message=\"トレーニング完了 \\n \"\n","                +\"Data:\"+str(dt_now)+\"\\n\"\n","                +\"ENV:\"+str(ENV)+\"\\n\"\n","                +\"CRITERION:\"+str(CRITERION)+\"\\n\"\n","                +\"GAMMA:\"+str(GAMMA)+\"\\n\"\n","                +\"LEARNING_RATE:\"+str(LEARNING_RATE)+\"\\n\"\n","                +\"MAX_STEPS:\"+str(MAX_STEPS)+\"\\n\"\n","                +\"MAX_STEPS:\"+str(NUM_EPISODES)+\"\\n\"\n","                +\"BATCH_SIZE:\"+str(BATCH_SIZE)+\"\\n\"\n","                +\"CAPACITY:\"+str(CAPACITY)+\"\\n\"\n","                +\"UPDATE_FREQ:\"+str(UPDATE_FREQ)+\"\\n\"\n","                +\"NUM_NEURON:\"+str(MODELS)+\"\\n\"\n","                ,\n","        image='img/test.png'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# メモ\n","## 改造案\n","- 台形内にボール座標が含まれたら，Roombaの中心座標からの相対座標を返す．台形外なら-1．\n","- 中間層を増やす．"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMNmAtJlq2uaXmau3WQyn85","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
